{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae5da0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "790f94ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# This solves this issue: \"RuntimeError: MPS backend out of memory\" when sending vgg19 to device\n",
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c633e729",
   "metadata": {},
   "source": [
    "# Models and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e29191",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Params and Hyper Parameters:\n",
    "hidden_sizes:tuple = (2048, 512)\n",
    "learning_rate = 0.01\n",
    "epochs = 20\n",
    "folds = 5\n",
    "\n",
    "use_gpu_if_available = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f429f7c6",
   "metadata": {},
   "source": [
    "## Custom NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89c21e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hidden_layers(hidden_sizes:tuple[int]):\n",
    "    hidden_layers = []\n",
    "\n",
    "    before_size = hidden_sizes[0]\n",
    "\n",
    "    for hidden_size in hidden_sizes[1:]:\n",
    "        hidden_layers += [\n",
    "            nn.Linear(in_features=before_size, out_features=hidden_size) ,\n",
    "            nn.ReLU(),\n",
    "        ]\n",
    "        before_size = hidden_size\n",
    "\n",
    "    return hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec183003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# References https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html \n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size:int, hidden_sizes:tuple[int], output_size:int):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_sizes[0]),\n",
    "            nn.ReLU(),\n",
    "            *build_hidden_layers(hidden_sizes),\n",
    "            nn.Linear(hidden_sizes[-1], output_size),\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        # x = self.flatten(x)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a93885",
   "metadata": {},
   "source": [
    "## VGG Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7a9f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vgg19_model(input_size:int, hidden_sizes:tuple[int], output_size:int):\n",
    "    import torchvision.models as models\n",
    "\n",
    "    vgg19 = models.vgg19_bn(\n",
    "        # pretrained=False,\n",
    "        weights= None,\n",
    "        # If weights is None, this needs to be true:\n",
    "        init_weights = True, \n",
    "        # num_classes is ignored if we specify a classifier below:\n",
    "        num_classes=output_size,\n",
    "    )\n",
    "\n",
    "    # modify the final fully connected layer to get binary classification:\n",
    "    # num_features = vgg19.classifier[6].in_features\n",
    "    # vgg19.classifier[6] = torch.nn.Linear(num_features, output_size)\n",
    "\n",
    "    # vgg19.avgpool= nn.AdaptiveAvgPool2d(output_size=(2, 2))\n",
    "\n",
    "    # vgg19.classifier = nn.Sequential(\n",
    "    #     # 6 divisor is a magic number, literally:\n",
    "    #     nn.Linear(in_features=int(input_size / 6) , out_features=hidden_sizes[0]) ,\n",
    "    #     nn.ReLU(),\n",
    "\n",
    "    #     *build_hidden_layers(hidden_sizes),\n",
    "        \n",
    "    #     nn.Dropout(p=0.6),  # Do we need this here? 0.5 by default\n",
    "            \n",
    "    #     nn.Linear(in_features=hidden_sizes[-1] , out_features=output_size),\n",
    "    #     # nn.LogSoftmax(dim=1)  \n",
    "    # )\n",
    "\n",
    "    return vgg19\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219e2395",
   "metadata": {},
   "source": [
    "## Running images on non-CPU:\n",
    "\n",
    "Mac:\n",
    "Run `brew install libjpeg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17971498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Init a device with cuda or mps so that it can train faster\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "device: Literal['cuda'] | Literal['mps'] | Literal['cpu'] = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "device = device if use_gpu_if_available else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a90ff84",
   "metadata": {},
   "source": [
    "# Run Training and Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42c90b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from time import time\n",
    "\n",
    "\n",
    "def run_training(\n",
    "        dataset_loader:DataLoader,\n",
    "        model: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        epochs:int\n",
    "        ):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_accuracies = []\n",
    "    losses = []\n",
    "\n",
    "    start_time = time()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Pass data to model:\n",
    "        number_of_samples = len(dataset_loader)\n",
    "        counter = 1\n",
    "        for train_x, train_y in dataset_loader:\n",
    "            print(f\"Training {counter} of {number_of_samples}, epoch {epoch}\")\n",
    "            counter += 1\n",
    "            train_x = train_x.to(device)\n",
    "            train_y = train_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(train_x)\n",
    "            loss = criterion(outputs, train_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += train_y.size(0)\n",
    "            correct += (predicted == train_y).sum().item()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        loss = running_loss / len(dataset_loader)\n",
    "        losses.append(loss)\n",
    "\n",
    "        train_accuracy = (correct / total) * 100\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}, Train Accuracy: {train_accuracy:.2f}%. Took {time()-start_time} seconds')\n",
    "        \n",
    "        start_time = time()\n",
    "\n",
    "    return train_accuracies, losses\n",
    "\n",
    "def run_test(test_loader: DataLoader, model: nn.Module,):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for test_x, test_y in test_loader:\n",
    "            test_x = test_x.to(device)\n",
    "            test_y = test_y.to(device)\n",
    "\n",
    "            outputs = model(test_x)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += test_y.size(0)\n",
    "            correct += (predicted == test_y).sum().item()\n",
    "    \n",
    "    accuracy = (correct / total) * 100\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\\n\")\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f59bc33",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24fe27bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shehab/anaconda3/envs/nn/lib/python3.12/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/shehab/anaconda3/envs/nn/lib/python3.12/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <367D4265-B20F-34BD-94EB-4F3EE47C385B> /Users/shehab/anaconda3/envs/nn/lib/python3.12/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/Users/shehab/anaconda3/envs/nn/lib/python3.12/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/shehab/anaconda3/envs/nn/lib/python3.12/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/shehab/anaconda3/envs/nn/lib/python3.12/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/shehab/anaconda3/envs/nn/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "%run ./dataset_load.py\n",
    "\n",
    "is_load_from_csv = input(\"Load from CSV or images directly? Type anything to load csv, leave blank for images.\")\n",
    "\n",
    "if is_load_from_csv: dataset = load_from_csv(device) # type: ignore\n",
    "else: dataset = load_from_images(device) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e4f4d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150528\n"
     ]
    }
   ],
   "source": [
    "number_of_features = dataset[0][0].numel()\n",
    "\n",
    "print(number_of_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52710dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 2 ['defected', 'not_defected']\n"
     ]
    }
   ],
   "source": [
    "number_of_classes = len(dataset.classes)\n",
    "print(\"Number of classes:\", number_of_classes, dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fe5f75",
   "metadata": {},
   "source": [
    "# K-Fold and running the training and validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c3248a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1. Training samples 1133, Testing samples 284\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU(inplace=True)\n",
      "    (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (38): ReLU(inplace=True)\n",
      "    (39): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU(inplace=True)\n",
      "    (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (45): ReLU(inplace=True)\n",
      "    (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (48): ReLU(inplace=True)\n",
      "    (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (51): ReLU(inplace=True)\n",
      "    (52): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Training 1 of 9, epoch 0\n",
      "Training 2 of 9, epoch 0\n",
      "Training 3 of 9, epoch 0\n",
      "Training 4 of 9, epoch 0\n",
      "Training 5 of 9, epoch 0\n",
      "Training 6 of 9, epoch 0\n",
      "Training 7 of 9, epoch 0\n",
      "Training 8 of 9, epoch 0\n",
      "Training 9 of 9, epoch 0\n",
      "Epoch 1/20, Loss: 5437.4265, Train Accuracy: 52.16%. Took 533.2084181308746 seconds\n",
      "Training 1 of 9, epoch 1\n",
      "Training 2 of 9, epoch 1\n",
      "Training 3 of 9, epoch 1\n",
      "Training 4 of 9, epoch 1\n",
      "Training 5 of 9, epoch 1\n",
      "Training 6 of 9, epoch 1\n",
      "Training 7 of 9, epoch 1\n",
      "Training 8 of 9, epoch 1\n",
      "Training 9 of 9, epoch 1\n",
      "Epoch 2/20, Loss: 104.0186, Train Accuracy: 65.93%. Took 737.7222456932068 seconds\n",
      "Training 1 of 9, epoch 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_subset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_subset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 37\u001b[0m train_accuracies, train_losses \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m run_test(test_loader, model)\n\u001b[1;32m     47\u001b[0m test_accuracies[fold_index] \u001b[38;5;241m=\u001b[39m test_accuracy\n",
      "Cell \u001b[0;32mIn[8], line 36\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(dataset_loader, model, optimizer, epochs)\u001b[0m\n\u001b[1;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, train_y)\n\u001b[1;32m     35\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 36\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_y\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nn/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nn/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/envs/nn/lib/python3.12/site-packages/torch/optim/adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    214\u001b[0m         group,\n\u001b[1;32m    215\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         state_steps,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/nn/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nn/lib/python3.12/site-packages/torch/optim/adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nn/lib/python3.12/site-packages/torch/optim/adam.py:379\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    378\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 379\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    382\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "test_accuracies = [0.0]*folds\n",
    "kfold = KFold(n_splits=folds, shuffle=True)\n",
    "fold = 1\n",
    "colors = 'bgcmk'\n",
    "for train_idx, test_idx in kfold.split(dataset):\n",
    "    print(f\"Fold {fold}. Training samples {len(train_idx)}, Testing samples {len(test_idx)}\")\n",
    "    \n",
    "    fold_index = fold-1\n",
    "\n",
    "    # Init a fresh model\n",
    "    # model = NeuralNetwork(\n",
    "    #     input_size=number_of_features, \n",
    "    #     hidden_size=hidden_size, \n",
    "    #     output_size=num_classes\n",
    "    # )\n",
    "    model = get_vgg19_model(\n",
    "        number_of_features, \n",
    "        hidden_sizes, \n",
    "        number_of_classes\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    \n",
    "    # Create train and test subsets\n",
    "    train_subset = Subset(dataset, train_idx.tolist())\n",
    "    test_subset = Subset(dataset, test_idx.tolist())\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=128, shuffle=True)\n",
    "    test_loader = DataLoader(test_subset, batch_size=128, shuffle=False)\n",
    "\n",
    "    train_accuracies, train_losses = run_training(\n",
    "        train_loader, \n",
    "        model, \n",
    "        optimizer, \n",
    "        epochs\n",
    "    )\n",
    "\n",
    "    test_accuracy = run_test(test_loader, model)\n",
    "\n",
    "    \n",
    "    test_accuracies[fold_index] = test_accuracy\n",
    "\n",
    "    # Plot for this fold\n",
    "    fig, left_y_axis = plt.subplots()\n",
    "    right_y_axis = left_y_axis.twinx()\n",
    "    top_x_axis = right_y_axis.twiny()\n",
    "\n",
    "    left_y_axis.plot(range(1, epochs + 1), train_losses, colors[fold_index]+'-',  label=f'Fold {fold} Train Loss')\n",
    "\n",
    "    top_x_axis.plot(fold, test_accuracy, colors[fold_index]+'o', label=f'Fold {fold} Test Accuracy')\n",
    "\n",
    "    right_y_axis.plot(range(1, epochs + 1), train_accuracies, colors[fold_index]+'--', label=f'Fold {fold} Train Accuracy')\n",
    "\n",
    "    plt.title('Training Loss and Accuracy Per Fold')\n",
    "\n",
    "    left_y_axis.set_xlabel('Epoch')\n",
    "    left_y_axis.set_ylabel('Loss')\n",
    "\n",
    "    right_y_axis.set_ylabel('Accuracy (%)')\n",
    "    right_y_axis.set_ylim(50.0, 100.5)\n",
    "\n",
    "    top_x_axis.set_xlim(0, folds+1)\n",
    "    top_x_axis.set_xticks(range(folds+1))\n",
    "    top_x_axis.set_xlabel(\"Fold Number\")\n",
    "    fig.legend( bbox_to_anchor=(0.9,0.1))\n",
    "    fig.savefig(f'training_loss_fold{fold}.png', bbox_inches='tight') \n",
    "\n",
    "    fold += 1\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter([f+1 for f in range(folds)], test_accuracies, color='purple', s=100, label='Test Accuracy')\n",
    "plt.xlabel('Fold Number')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlim(0, folds+1)\n",
    "plt.xticks(range(folds+1))\n",
    "\n",
    "plt.title('Test Accuracy Per Fold')\n",
    "\n",
    "plt.savefig('test_accuracy.png', bbox_inches='tight') \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
